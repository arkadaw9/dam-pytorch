{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from fastprogress import master_bar, progress_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\text{Data Creation Module}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(k, d, N, return_rank=False):\n",
    "    mean_k = np.zeros(k)\n",
    "    cov_k = np.eye(k)\n",
    "\n",
    "    x = np.random.multivariate_normal(mean_k, cov_k, N)\n",
    "    x = torch.tensor(x).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mat = NN(k, d)(x)\n",
    "        \n",
    "    if return_rank:\n",
    "        from numpy.linalg import matrix_rank\n",
    "        rank = matrix_rank(mat.detach().cpu().numpy())\n",
    "        return mat, rank\n",
    "    else:\n",
    "        return mat\n",
    "\n",
    "# configs\n",
    "k = 20\n",
    "N = 5000\n",
    "d = 100\n",
    "\n",
    "result_path = './results/multi_gaussian/var_%d/mlp/' % k\n",
    "\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path)\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\text{Architecture}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(NN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, out_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAM(nn.Module):\n",
    "    \"\"\" Discriminative Amplitude Modulator Layer (1-D) \"\"\"\n",
    "    def __init__(self, in_dim):\n",
    "        super(DAM, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        \n",
    "        self.mu = torch.arange(self.in_dim).float() / self.in_dim * 5.0\n",
    "        self.mu = nn.Parameter(self.mu, requires_grad=False)\n",
    "        self.beta = nn.Parameter(torch.ones(1), requires_grad=True)\n",
    "        self.alpha = nn.Parameter(torch.ones(1), requires_grad=False)\n",
    "        self.register_parameter('mu', self.mu)\n",
    "        self.register_parameter('beta', self.beta)\n",
    "        self.register_parameter('alpha', self.alpha)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * self.mask()\n",
    "    \n",
    "    def mask(self):\n",
    "        return self.relu(self.tanh((self.alpha ** 2) * (self.mu + self.beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEnc(torch.nn.Module):\n",
    "    def __init__(self, num_neuron):\n",
    "        super(AEnc, self).__init__()\n",
    "        \n",
    "        self.num_neuron = num_neuron\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.num_neuron, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 50)\n",
    "        )\n",
    "        self.decoder = NN(50, self.num_neuron)\n",
    "        self.dam_layer = DAM(50)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        h = self.dam_layer(out)\n",
    "        x_r = self.decoder(h)\n",
    "        return x_r, h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\text{Training Module}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = master_bar(range(1, 6))\n",
    "\n",
    "for run in mb:\n",
    "    lambda_r = 0.1\n",
    "    compact_dim = []\n",
    "    net = AEnc(d).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    pb = progress_bar(range(10000), parent=mb)\n",
    "    mb.names = ['layer Embd']\n",
    "    x_bounds = [0, len(net.dam_layer.mask())+1]\n",
    "    y_bounds = [0, 1]\n",
    "    x_n = np.arange(len(net.dam_layer.mask()))\n",
    "    y1 = net.dam_layer.mask().detach().cpu().numpy()\n",
    "    graphs = [[x_n,y1],]\n",
    "    mb.update_graph(graphs, x_bounds, y_bounds)\n",
    "    print(\"[Epoch\\tloss\\tMSE\\tReg\\tbeta_1]\")\n",
    "\n",
    "    \n",
    "    x, rank = get_data(k, d, N, return_rank=True)\n",
    "    x = x.float().to(device)\n",
    "    \n",
    "    for epoch in pb:\n",
    "        optimizer.zero_grad()\n",
    "        x_rc, _ = net(x)\n",
    "        beta_1 = net.dam_layer.beta\n",
    "        loss_data = criterion(x_rc, x)\n",
    "        loss = loss_data + lambda_r * beta_1\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        btl_dim = (net.dam_layer.mask() != 0).sum().item()\n",
    "        compact_dim.append(btl_dim)\n",
    "\n",
    "        if epoch % 10 == 0:       \n",
    "            y1 = net.dam_layer.mask().detach().cpu().numpy()\n",
    "            graphs = [[x_n,y1],]\n",
    "            mb.update_graph(graphs, x_bounds, y_bounds)\n",
    "\n",
    "        sys.stdout.write(\"\\r[%d\\t%.5e\\t%.5e\\t%.3f]\" % (epoch, loss.item(), loss_data.item(),  net.dam_layer.beta.item()))\n",
    "        \n",
    "\n",
    "    print('\\nFinal Embedding dim:', btl_dim)\n",
    "\n",
    "    torch.save({\n",
    "        'state_dict': net.state_dict(),\n",
    "        'btn_dim': compact_dim,\n",
    "        'rank': rank\n",
    "    }, os.path.join(result_path, 'run%d.pt' % run))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
