{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from fastprogress import master_bar, progress_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(k, d, N, return_rank=False):\n",
    "    mean_k = np.zeros(k)\n",
    "    cov_k = np.eye(k)\n",
    "\n",
    "    x = np.random.multivariate_normal(mean_k, cov_k, N)\n",
    "    x = torch.tensor(x).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mat = QResLayer(k, d)(x)\n",
    "        \n",
    "    if return_rank:\n",
    "        from numpy.linalg import matrix_rank\n",
    "        rank = matrix_rank(mat.detach().cpu().numpy())\n",
    "        return mat, rank\n",
    "    else:\n",
    "        return mat\n",
    "\n",
    "# configs\n",
    "k = 20\n",
    "N = 5000\n",
    "d = 100\n",
    "\n",
    "result_path = './results/multi_gaussian/var_%d/quad/' % k\n",
    "\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path)\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: \n",
    "# https://github.com/jayroxis/qres/blob/master/QRes/discussion/sin_wave/models.py\n",
    "\n",
    "class QResLayer(nn.Module):\n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight: torch.Tensor\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True) -> None:\n",
    "        super(QResLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight_1 = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.weight_2 = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.kaiming_uniform_(self.weight_1, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.weight_2, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight_1)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        h_1 = F.linear(input, self.weight_1, bias=None)\n",
    "        h_2 = F.linear(input, self.weight_2, bias=None)\n",
    "        return torch.add(\n",
    "            torch.mul(h_1, h_2), \n",
    "            F.linear(input, self.weight_1, self.bias)\n",
    "        )\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )\n",
    "    \n",
    "nn.QResLayer = QResLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAM(nn.Module):\n",
    "    \"\"\" Discriminative Amplitude Modulator Layer (1-D) \"\"\"\n",
    "    def __init__(self, in_dim):\n",
    "        super(DAM, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        \n",
    "        self.mu = torch.arange(self.in_dim).float() / self.in_dim * 5.0\n",
    "        self.mu = nn.Parameter(self.mu, requires_grad=False)\n",
    "        self.beta = nn.Parameter(torch.ones(1) * 5, requires_grad=True)\n",
    "        self.alpha = nn.Parameter(torch.ones(1), requires_grad=False)\n",
    "        self.register_parameter('mu', self.mu)\n",
    "        self.register_parameter('beta', self.beta)\n",
    "        self.register_parameter('alpha', self.alpha)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * self.mask()\n",
    "    \n",
    "    def mask(self):\n",
    "        return self.relu(self.tanh((self.alpha ** 2) * (self.mu + self.beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEnc(torch.nn.Module):\n",
    "    def __init__(self, num_neuron):\n",
    "        super(AEnc, self).__init__()\n",
    "        \n",
    "        self.num_neuron = num_neuron\n",
    "        self.enc_layer_1 = nn.Linear(self.num_neuron, 256)\n",
    "        self.enc_layer_2 = nn.Linear(256, 256)\n",
    "        self.enc_layer_3 = nn.Linear(256, 50)\n",
    "        \n",
    "        self.dam_layer = DAM(50)\n",
    "        self.dec_layer = nn.QResLayer(50, self.num_neuron)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.elu = nn.ELU()\n",
    "        self.sin = lambda x: torch.sin(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.leakyrelu(self.enc_layer_1(x))\n",
    "        out = self.leakyrelu(self.enc_layer_2(out))\n",
    "        out = self.enc_layer_3(out)\n",
    "        h = self.dam_layer(out)\n",
    "        x_r = self.dec_layer(h)\n",
    "        return x_r, h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\text{Training Module}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = master_bar(range(1, 6))\n",
    "\n",
    "for run in mb:\n",
    "    lambda_r = 0.01\n",
    "    compact_dim = []\n",
    "    net = AEnc(d).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.01, weight_decay=1e-6)\n",
    "\n",
    "    pb = progress_bar(range(5000), parent=mb)\n",
    "    mb.names = ['layer Embd']\n",
    "    x_bounds = [0, len(net.dam_layer.mask())+1]\n",
    "    y_bounds = [0, 1]\n",
    "    x_n = np.arange(len(net.dam_layer.mask()))\n",
    "    y1 = net.dam_layer.mask().detach().cpu().numpy()\n",
    "    graphs = [[x_n,y1],]\n",
    "    mb.update_graph(graphs, x_bounds, y_bounds)\n",
    "    print(\"[Epoch\\tloss\\tMSE\\tReg\\tbeta_1]\")\n",
    "\n",
    "    \n",
    "    x, rank = get_data(k, d, N, return_rank=True)\n",
    "    x = x.float().to(device)\n",
    "    \n",
    "    for epoch in pb:\n",
    "        optimizer.zero_grad()\n",
    "        x_rc, _ = net(x)\n",
    "        beta_1 = net.dam_layer.beta\n",
    "        loss_data = criterion(x_rc, x)\n",
    "        loss = loss_data + lambda_r * beta_1\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        btl_dim = (net.dam_layer.mask() != 0).sum().item()\n",
    "        compact_dim.append(btl_dim)\n",
    "\n",
    "        if epoch % 10 == 0:       \n",
    "            y1 = net.dam_layer.mask().detach().cpu().numpy()\n",
    "            graphs = [[x_n,y1],]\n",
    "            mb.update_graph(graphs, x_bounds, y_bounds)\n",
    "\n",
    "        sys.stdout.write(\"\\r[%d\\t%.5e\\t%.5e\\t%.3f]\" % (epoch, loss.item(), loss_data.item(),  net.dam_layer.beta.item()))\n",
    "        \n",
    "\n",
    "    print('\\nFinal Embedding dim:', btl_dim)\n",
    "\n",
    "    torch.save({\n",
    "        'state_dict': net.state_dict(),\n",
    "        'btn_dim': compact_dim,\n",
    "        'rank': rank\n",
    "    }, os.path.join(result_path, 'run%d.pt' % run))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
